{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Sheet 7\n",
    "\n",
    "- To be completed by **4pm** on **Fri 22nd Mar** and uploaded to [Problem Sheet 7 submission point](https://moodle.bath.ac.uk/mod/assign/view.php?id=1211639) on Moodle.\n",
    "\n",
    "## Gradient Descent (GD) and $\\beta$-smoothness of functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (a) (Warm-up): upper bound of the next loss in GD method\n",
    "\n",
    "- Prove Lemma 4.19 in lecture notes: if $L$ is $\\beta$-smooth on $\\mathbb{R}^n$, $\\boldsymbol\\theta_k$ is the current iterate in GD, and $t_k>0$ is the current learning rate, then\n",
    "$$\n",
    "L(\\boldsymbol\\theta_{k+1}) \\le L(\\boldsymbol\\theta_k) - t_k \\left(1-t_k \\frac{\\beta}{2} \\right) \\|\\nabla L(\\boldsymbol\\theta_k)\\|_2^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (b): optimal constant learning rate in GD method \n",
    "\n",
    "- Under the assumptions of Task (a), calculate the optimal constant learning rate $t_k = \\hat t > 0$ which minimises the **upper bound** of $L(\\boldsymbol\\theta_{k+1})$ derived above (for the fixed $\\boldsymbol\\theta_k$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (c) (Warm-up): $\\beta$-smoothness\n",
    "Consider the disk domain $\\Omega_r = \\{\\boldsymbol\\theta = (\\theta_1,\\theta_2): \\|\\boldsymbol\\theta\\|_2 \\le r\\} \\subset \\mathbb{R}^2$ for some $r>0$, and the function $L(\\boldsymbol\\theta) = \\theta_1^2 + 3 \\theta_2^2 + \\theta_2^4.$ \n",
    "- Find whether $L$ is $\\beta$-smooth on $\\Omega_r$, and if yes, suggest a value of $\\beta>0$ as a function of $r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (d): calculation of the optimal constant learning rate\n",
    "Consider $\\boldsymbol\\theta = (\\theta_1,\\theta_2) \\in \\mathbb{R}^2$ and the function $L(\\boldsymbol\\theta) = \\theta_1^2 + 3 \\theta_2^2 + \\theta_2.$ \n",
    "- Calculate the optimal constant learning rate $\\hat t$ for GD method.\n",
    "\n",
    "_Hint: use Task (b) and_ $\\beta$-_smoothness of_ $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Loss gradient for perturbed polynomial data regression\n",
    "Recall Problem Sheet 2 code that creates a perturbed cubic polynomial data $(\\mathbf{x},\\mathbf{y})=$(`X`,`Y`) and the Vandermonde feature matrix $V$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt    \n",
    "\n",
    "def TrueSample():\n",
    "    x = np.random.uniform(-1,1)\n",
    "    y = x - x**3 + 0.1*np.random.randn()\n",
    "    return x,y\n",
    "\n",
    "def features(x,n):\n",
    "    powers = np.arange(n+1)               # [0,1,2,...,n]\n",
    "    powers = np.reshape(powers, (1, -1))  # Make it explicitly a row vector\n",
    "    x = np.reshape(x, (-1, 1))            # Make it explicitly a column vector\n",
    "    return x**powers                      # Python automatically broadcasts the vectors to each others' shapes \n",
    "                                          # and takes the power between the resulting matrices elementwise\n",
    "\n",
    "Nsamples = 30\n",
    "Y = np.zeros(Nsamples)\n",
    "X = np.zeros(Nsamples)\n",
    "for i in range(Y.size):\n",
    "    X[i],Y[i] = TrueSample()\n",
    "\n",
    "n = 3\n",
    "V = features(X,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write Python functions `L(theta,V,Y)` and `gL(theta,V,Y)` with arguments `theta`$=\\boldsymbol\\theta\\in \\mathbb{R}^{n+1}$, `V`$=V\\in\\mathbb{R}^{m \\times (n+1)}$ and `Y`$=\\mathbf{y}\\in\\mathbb{R}^{m}$, which compute and return the mean squared error loss function \n",
    "$$\n",
    "L(\\boldsymbol\\theta) = \\frac{1}{m}\\left\\|V\\boldsymbol\\theta - \\mathbf{y}\\right\\|_2^2\n",
    "$$ \n",
    "and its gradient $\\nabla L(\\boldsymbol\\theta) = A\\boldsymbol\\theta - \\mathbf{b}$, respectively, where $A\\in\\mathbb{R}^{(n+1) \\times (n+1)}$ and $\\mathbf{b} \\in \\mathbb{R}^{n+1}$ are the matrix and the vector you should identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: GD for perturbed polynomial data regression\n",
    "- Find $\\boldsymbol\\theta^* = \\arg\\min L(\\boldsymbol\\theta)$ for $L(\\boldsymbol\\theta)$ defined in Task 1 using the `optimise_loss` function from Problem Sheet 2.\n",
    "- Copy over the gradient descent function `gd` (with `fminbound` line search) implemented in `GD.ipynb` in week6.\n",
    "- For each `eps` $=\\varepsilon\\in\\{10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}\\}$, \n",
    "apply `gd` with the chosen `eps` and  `max_iters`$=10^4$ to compute $\\boldsymbol\\theta_k \\approx \\boldsymbol\\theta^* = \\arg\\min L(\\boldsymbol\\theta)$ for $L(\\boldsymbol\\theta)$ defined in Task 1, starting from $\\boldsymbol\\theta_0 = \\mathbf{0}$. For each $\\varepsilon$, record the number of iterations $k_{\\varepsilon}$ at which `gd` converges, and the error $\\|\\boldsymbol\\theta_{k_{\\varepsilon}} - \\boldsymbol\\theta^*\\|_2$.\n",
    "- Plot $\\|\\boldsymbol\\theta_{k_{\\varepsilon}} - \\boldsymbol\\theta^*\\|_2$ in the logarithmic scale as a function of the number of iterations $k_{\\varepsilon}$ at which `gd` converges for each $\\varepsilon$.\n",
    "- What kind of convergence you can infer from this plot?\n",
    "\n",
    "_Hint: you may find the_ [`matplotlib.pyplot.semilogy`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.semilogy.html) _function useful._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
