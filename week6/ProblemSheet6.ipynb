{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Sheet 6\n",
    "\n",
    "- To be completed by **4pm** on **Fri 15th Mar** and uploaded to [Problem Sheet 6 submission point](https://moodle.bath.ac.uk/mod/assign/view.php?id=1208731) on Moodle.\n",
    "\n",
    "## Halfspaces classifier and logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (a) (Warm-up)\n",
    "Consider $\\mathbf{D} = \\{(\\mathbf{x}_1,y_1), (\\mathbf{x}_2,y_2)\\}$, where\n",
    "$$\n",
    "\\mathbf{x}_1 = \\begin{bmatrix}-1 \\\\ 1\\end{bmatrix}, \\qquad \\mathbf{x}_2 = \\begin{bmatrix}2 \\\\ -1\\end{bmatrix},\n",
    "$$\n",
    "and $y_1=1$, $y_2=-1$.\n",
    "\n",
    "- Find all halfspaces parameters $\\boldsymbol\\theta^* \\in \\mathbb{R}^2$ solving Eq. (4.4) in the typed notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (b)\n",
    "Consider the same dataset as in Task (a).\n",
    "\n",
    "- Find the Support Vector Machine parameter $\\boldsymbol\\theta^* \\in \\mathbb{R}^2$ solving Eq. (4.5) in the typed notes.\n",
    "- Explain why it should be preferred to $\\boldsymbol\\theta^*$ from Task (a) for reliable prediction on new data.\n",
    "\n",
    "_Hint: similarly to Task (a), consider two cases_ $\\theta_1 \\ge 0$ _and_ $\\theta_1<0$ _separately. However, now $\\theta_2$ should depend uniquely on_ $\\theta_1$ _in each case._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (c)\n",
    "Consider a dataset $\\mathbf{D} = \\{(\\mathbf{x}_1,y_1), \\ldots, (\\mathbf{x}_m,y_m)\\}$ with $\\mathbf{x}_i \\in \\mathbb{R}^n$, $y_i \\in \\{-1,1\\}$, $i=1,\\ldots,m$.\n",
    "- Find the gradient of the minus-log-likelihood of the logistic regression of this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: logistic regression\n",
    "\n",
    "Consider a larger version of the dataset from Task (a):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.vstack([np.linspace(-3,3,9).reshape(-1,1) + np.array([0, 1]),\n",
    "               np.linspace(-3,3,9).reshape(-1,1) + np.array([0,-1])])\n",
    "y = np.hstack((np.ones(9), -np.ones(9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a Python function `L(theta,X,y)` that takes as input a parameter vector `theta`=$\\boldsymbol\\theta\\in\\mathbb{R}^n$, a matrix `X`$\\in\\mathbb{R}^{m \\times n}$ of row vectors $\\mathbf{x}_1,\\ldots,\\mathbf{x}_m \\in \\mathbb{R}^n$ stacked vertically, and a vector of labels `y`=$(y_1,\\ldots,y_m)$, and returns the minus-log-likelihood $L_{\\mathbf{D}}(\\boldsymbol\\theta)$ of the logistic regression of the dataset $\\mathbf{D}$=(`X`,`y`) at the parameter `theta`.\n",
    "- Write a Python function `gL(theta,X,y)` that takes the same inputs as `L(theta,X,y)`, and returns the gradient vector $\\nabla L_{\\mathbf{D}}(\\boldsymbol\\theta)$ derived in Task (c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "- Read about [scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) function.\n",
    "- Write a Python code using this function to minimize $L_{\\mathbf{D}}$ from Task 1, starting from a zero initial guess.\n",
    "- Write a Python code to plot $\\mathbf{x}_1,\\ldots,\\mathbf{x}_m$ from Task 1 as points in $\\mathbb{R}^2$, and the separating line corresponding to $\\boldsymbol\\theta^*$ produced by scipy.optimize.minimize on the same plot. You may want to restrict `xlim` and `ylim` of the plot to $[-4,4]$ for convenient picture.\n",
    "- How accurate is the separating line visually, and how small is $\\|\\nabla L_{\\mathbf{D}}(\\boldsymbol\\theta^*)\\|_2$?\n",
    "\n",
    "_Hint: the separating line equation reads_ $\\langle \\boldsymbol\\theta^*, \\mathbf{x}\\rangle = 0$, _so the points on the line can be calculated as_ $\\mathbf{x} = t \\left[\\theta_2^*, -\\theta_1^*\\right]$ _for_ $t\\in\\mathbb{R}$, _which you can restrict to_ $t \\in [-1,1]$ _for plotting._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "- Copy over the Gradient Descent function `gd` from the `GD.ipynb` demonstration notebook here.\n",
    "- Repeat computations of Task 2 but replacing `scipy.optimize.minimize` with `gd` to find $\\boldsymbol\\theta^*$.\n",
    "- What happens? How accurate is the separating line visually, and how small is $\\|\\nabla L_{\\mathbf{D}}(\\boldsymbol\\theta^*)\\|_2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
